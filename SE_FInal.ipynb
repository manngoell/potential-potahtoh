{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b20374a2-335f-40c9-9d7a-6091f07b4795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[**] Reading Excel file: 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Master WLP L0 Super Collector as on 23-SEP-2025.xlsx'\n",
      "[*] Cutting the 38th column (AL)...\n",
      "[*] Pasting the cut column between Q and R...\n",
      "[*] Saving the modified file to: 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mangoel\\AppData\\Local\\Temp\\ipykernel_28336\\3619529699.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(r_index, 'AL', al_column_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] File saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def cut_and_paste_column(input_path: str, output_path: str):\n",
    "    try:\n",
    "        print(f\"[**] Reading Excel file: '{input_path}'\")\n",
    "        df = pd.read_excel(input_path, engine='openpyxl')\n",
    "\n",
    "        if df.shape[1] < 38:\n",
    "            print(\"[ERROR] The input file does not have enough columns (needs at least 38).\")\n",
    "            return\n",
    "\n",
    "        # Cut the 38th column (AL)\n",
    "        print(\"[*] Cutting the 38th column (AL)...\")\n",
    "        al_column_data = df.pop(df.columns[37])\n",
    "\n",
    "        r_index = 17\n",
    "\n",
    "        print(\"[*] Pasting the cut column between Q and R...\")\n",
    "        df.insert(r_index, 'AL', al_column_data)\n",
    "\n",
    "        # Save the modified DataFrame to a new Excel file\n",
    "        print(f\"[*] Saving the modified file to: '{output_path}'\")\n",
    "        df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "        print(\"[SUCCESS] File saved successfully.\")\n",
    "    \n",
    "    except FileNotFoundError: # will only happen is the file path is not found.\n",
    "        print(f\"[ERROR] The file was not found: {input_path}\")\n",
    "    except Exception as e: # will only happen when the file is opened somewhere.\n",
    "        print(f\"[ERROR] An error occurred:(file is opened somewhere) {e}\")\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths with your actual file paths\n",
    "    input_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Master WLP L0 Super Collector as on 23-SEP-2025.xlsx\"\n",
    "    output_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    \n",
    "    cut_and_paste_column(input_path=input_excel_file, output_path=output_excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eda276ca-ce0d-4b27-b0ce-6db006466162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Reading Excel file without a default header: 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n",
      "[*] Original headers found: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 'CLIENTDETAILS-1', nan, nan, nan, nan, 'CLIENTDETAILS-2', nan, nan, nan, nan, 'CLIENTDETAILS-3', nan, nan, nan, nan, 'CLIENTDETAILS-4', nan, nan, nan, nan, nan, nan]\n",
      "[*] Headers after de-duplication: [nan, 'nan_1', 'nan_2', 'nan_3', 'nan_4', 'nan_5', 'nan_6', 'nan_7', 'nan_8', 'nan_9', 'nan_10', 'nan_11', 'nan_12', 'nan_13', 'nan_14', 'nan_15', 'nan_16', 'CLIENTDETAILS-1', 'nan_17', 'nan_18', 'nan_19', 'nan_20', 'CLIENTDETAILS-2', 'nan_21', 'nan_22', 'nan_23', 'nan_24', 'CLIENTDETAILS-3', 'nan_25', 'nan_26', 'nan_27', 'nan_28', 'CLIENTDETAILS-4', 'nan_29', 'nan_30', 'nan_31', 'nan_32', 'nan_33', 'nan_34']\n",
      "[*] Successfully loaded 2654 rows and 39 columns.\n",
      "[*] Processing Block 1 (Source: 'Source Site')...\n",
      "[*] Processing Block 2 (Source: 'Source Site')...\n",
      "[*] Processing Block 3 (Source: 'Source Site')...\n",
      "[*] Processing Block 4 (Source: 'Source Site')...\n",
      "[*] Stacking the processed blocks...\n",
      "[*] Transformation complete. New file will have 10616 rows.\n",
      "[SUCCESS] The transformed file has been saved as 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def _mangle_duplicate_columns(columns):\n",
    "    counts = defaultdict(int)\n",
    "    new_columns = []\n",
    "    for col in columns:\n",
    "        if not col or str(col).isspace():\n",
    "            col = 'Unnamed'\n",
    "        if counts[col] > 0:\n",
    "            new_columns.append(f\"{col}_{counts[col]}\")\n",
    "        else:\n",
    "            new_columns.append(col)\n",
    "        counts[col] += 1\n",
    "    return new_columns\n",
    "\n",
    "def transform_client_details_robust_v2(input_path: str, output_path: str):\n",
    "    try:\n",
    "        print(f\"[*] Reading Excel file without a default header: '{input_path}'\")\n",
    "        full_df = pd.read_excel(input_path, header=None, usecols=range(1, 40), engine='openpyxl')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] The file was not found: {input_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred while reading the Excel file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    super_headers = full_df.iloc[3].ffill().tolist() \n",
    "    original_headers = full_df.iloc[4].tolist() \n",
    "    \n",
    "    print(\"[*] Original headers found:\", original_headers)\n",
    "    main_headers = _mangle_duplicate_columns(original_headers)\n",
    "    print(\"[*] Headers after de-duplication:\", main_headers)\n",
    "\n",
    "    df = full_df.iloc[5:].copy()\n",
    "    df.columns = main_headers\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f\"[*] Successfully loaded {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    \n",
    "    static_cols = main_headers[:17] \n",
    "    base_repeated_names = _mangle_duplicate_columns(original_headers[16:21])\n",
    "    \n",
    "    processed_dfs = []\n",
    "\n",
    "    for i in range(4):\n",
    "        start_col_index = 17 + (i * 5)\n",
    "        end_col_index = start_col_index + 5\n",
    "        \n",
    "        source_block_name = super_headers[start_col_index]\n",
    "        print(f\"[*] Processing Block {i+1} (Source: '{source_block_name}')...\")\n",
    "        \n",
    "        current_block_cols = main_headers[start_col_index:end_col_index]\n",
    "        temp_df = df[static_cols + current_block_cols].copy()\n",
    "        \n",
    "        rename_mapping = dict(zip(current_block_cols, base_repeated_names))\n",
    "        temp_df.rename(columns=rename_mapping, inplace=True)\n",
    "        processed_dfs.append(temp_df)\n",
    "\n",
    "    print(\"[*] Stacking the processed blocks...\")\n",
    "    final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    final_df.dropna(subset=base_repeated_names, how='all', inplace=True)\n",
    "    \n",
    "    print(f\"[*] Transformation complete. New file will have {final_df.shape[0]} rows.\")\n",
    "    try:\n",
    "        final_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "        print(f\"[SUCCESS] The transformed file has been saved as '{output_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not save the file: {e}\")\n",
    "\n",
    "# --- How to use the script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Please ensure these paths are correct on your system\n",
    "    input_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    # Using a new output file name to avoid confusion\n",
    "    output_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    \n",
    "    transform_client_details_robust_v2(input_path=input_excel_file, output_path=output_excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "146a0eb3-13e3-4f47-844a-659d63e016dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Reading Excel file: 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n",
      "[*] Removing the header row (r=0)...\n",
      "[*] Saving the modified file to: 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n",
      "[SUCCESS] File saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_header_row(input_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Reads an Excel file, removes the header row (r=0), and saves the modified DataFrame to a new Excel file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the Excel file without treating the first row as the header\n",
    "        print(f\"[*] Reading Excel file: '{input_path}'\")\n",
    "        df = pd.read_excel(input_path, header=None, engine='openpyxl')  # Read without headers\n",
    "\n",
    "        # Remove the first row (header row)\n",
    "        print(\"[*] Removing the header row (r=0)...\")\n",
    "        df = df.iloc[1:].reset_index(drop=True)  # Skip the first row and reset the index\n",
    "\n",
    "        # Save the modified DataFrame to a new Excel file\n",
    "        print(f\"[*] Saving the modified file to: '{output_path}'\")\n",
    "        df.to_excel(output_path, index=False, header=False, engine='openpyxl')  # Save without headers\n",
    "        print(\"[SUCCESS] File saved successfully.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] The file was not found: {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# --- How to use the script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths with your actual file paths\n",
    "    input_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    output_excel_file =r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    \n",
    "    remove_header_row(input_path=input_excel_file, output_path=output_excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5fb369fc-1282-43f2-8047-81bff4986694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'. It has 22 columns.\n",
      "Error: The input file is missing the following required columns: ['AIMOTR/WL5e CLIENTPORT']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "INPUT_FILENAME = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "OUTPUT_FILENAME = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "\n",
    "#    You can add more columns here, e.g., ['COMMONPORT', 'Rate', 'CardType']\n",
    "COLUMNS_TO_COPY = ['OPSSH-SL']\n",
    "ADDITIONAL_MATCH_COLUMN = 'AIMOTR/WL5e CLIENTPORT'\n",
    "# -------------------------------\n",
    "\n",
    "def process_snc_protection_paths(input_path: str, output_path: str, columns_to_process: list, match_column: str):\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(input_path)\n",
    "        print(f\"Successfully loaded data from '{input_path}'. It has {df.shape[1]} columns.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_path}' was not found. Please check the INPUT_FILENAME.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return\n",
    "    required_columns = ['SNCName', match_column] + columns_to_process\n",
    "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: The input file is missing the following required columns: {missing_cols}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Process the Data ---\n",
    "    # Create a copy to store our changes. This is safer than modifying the original dataframe.\n",
    "    df_updated = df.copy()\n",
    "    update_count = 0\n",
    "    \n",
    "    # Filter for rows where 'SNCName' contains '-W-'\n",
    "    working_rows = df[df['SNCName'].str.contains('-W-', na=False)]\n",
    "    print(f\"Found {len(working_rows)} 'Working' paths to process.\")\n",
    "\n",
    "    # Iterate over each \"Working\" row found\n",
    "    for index, w_row in working_rows.iterrows():\n",
    "        source_snc = w_row['SNCName']\n",
    "        source_match_value = w_row[match_column]  # Value from the additional match column\n",
    "        \n",
    "        # Construct the \"Protection\" SNCName by replacing '-W-' with '-P-'\n",
    "        target_snc = source_snc.replace('-W-', '-P-', 1)\n",
    "\n",
    "        # Find the index of the matching \"Protection\" row in our copy\n",
    "        target_indices = df_updated.index[\n",
    "            (df_updated['SNCName'] == target_snc) & \n",
    "            (df_updated[match_column] == source_match_value)\n",
    "        ].tolist()\n",
    "\n",
    "        if not target_indices:\n",
    "            continue\n",
    "        \n",
    "        # Assuming SNCNames are unique and match_column ensures uniqueness, we take the first match\n",
    "        target_index = target_indices[0]\n",
    "\n",
    "        # Now, loop through the columns we need to copy for this matched pair\n",
    "        for column_name in columns_to_process:\n",
    "            value_to_copy = w_row[column_name]\n",
    "            \n",
    "            # Check if there is a value to copy (handles empty strings, None, NaN)\n",
    "            if pd.notna(value_to_copy) and value_to_copy != '':\n",
    "                # Copy the value to the target row and specified column\n",
    "                df_updated.loc[target_index, column_name] = value_to_copy\n",
    "                update_count += 1\n",
    "            else:\n",
    "                print(f\"     - Skipping column '{column_name}' as source value is empty.\")\n",
    "\n",
    "    # --- 4. Save the Result ---\n",
    "    # The df_updated DataFrame contains all original columns and data, plus our changes.\n",
    "    try:\n",
    "        df_updated.to_excel(output_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the file: {e}\")\n",
    "\n",
    "# This block runs the main function when the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Execute the main processing logic with the settings from the configuration section\n",
    "    process_snc_protection_paths(\n",
    "        input_path=INPUT_FILENAME,\n",
    "        output_path=OUTPUT_FILENAME,\n",
    "        columns_to_process=COLUMNS_TO_COPY,\n",
    "        match_column=ADDITIONAL_MATCH_COLUMN\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4434cfbd-820f-405c-89cb-387071ff7991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Reading Excel file: 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n",
      "[*] Creating new column: 'NE-SHELFSLOT-OPSSW'...\n",
      "[*] Saving the modified file to: 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'...\n",
      "[SUCCESS] File saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_combined_column(input_path: str, output_path: str, new_column_name: str):\n",
    "    \"\"\"\n",
    "    Reads an Excel file, looks into a specific sheet, and creates a new column\n",
    "    by combining values from the columns 'SOURCENE/OriginatingNode', 'OPSSH-SL', and 'OPSNO'.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to the input Excel file.\n",
    "        sheet_name (str): Name of the sheet to process.\n",
    "        output_path (str): Path to save the modified Excel file.\n",
    "        new_column_name (str): Name of the new column to be created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the specific sheet into a DataFrame\n",
    "        print(f\"[*] Reading Excel file: '{input_path}'\")\n",
    "        df = pd.read_excel(input_path, engine='openpyxl')\n",
    "\n",
    "        # Ensure the required columns exist\n",
    "        required_columns = [\"SOURCENE/OriginatingNode\", \"OPSSH-SL\", \"OPSNO\"]\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                print(f\"[ERROR] The column '{col}' does not exist in the sheet.\")\n",
    "                return\n",
    "\n",
    "        # Create the new column by combining the specified columns without spaces\n",
    "        print(f\"[*] Creating new column: '{new_column_name}'...\")\n",
    "        df[new_column_name] = (\n",
    "            df[\"SOURCENE/OriginatingNode\"].astype(str) +\n",
    "            df[\"OPSSH-SL\"].astype(str) +\n",
    "            df[\"OPSNO\"].astype(str)\n",
    "        )\n",
    "\n",
    "        # Save the modified DataFrame to a new Excel file\n",
    "        print(f\"[*] Saving the modified file to: '{output_path}'...\")\n",
    "        df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "        print(\"[SUCCESS] File saved successfully.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] The file was not found: {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# --- How to use the script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths and parameters with your actual file paths and column name\n",
    "    input_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    \n",
    "    output_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    new_column_name = \"NE-SHELFSLOT-OPSSW\"  # Replace with the name of the new column\n",
    "\n",
    "    create_combined_column(\n",
    "        input_path=input_excel_file,\n",
    "        output_path=output_excel_file,\n",
    "        new_column_name=new_column_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c94265d-5d9e-4eff-ab25-2459ee68752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Reading Excel file: 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n",
      "[*] Filtering rows based on 'CH TYPE'...\n",
      "[*] Writing filtered data to 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'...\n",
      "[SUCCESS] File saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_and_split_to_sheets(input_path: str, output_path: str, column_name: str, filter_value1, filter_value2):\n",
    "\n",
    "    try:\n",
    "        # Load the Excel file into a DataFrame\n",
    "        print(f\"[*] Reading Excel file: '{input_path}'\")\n",
    "        df = pd.read_excel(input_path, engine='openpyxl')\n",
    "\n",
    "        # Ensure the specified column exists\n",
    "        if column_name not in df.columns:\n",
    "            print(f\"[ERROR] The column '{column_name}' does not exist in the input file.\")\n",
    "            return\n",
    "\n",
    "        # Filter rows based on the specified values\n",
    "        print(f\"[*] Filtering rows based on '{column_name}'...\")\n",
    "        df_filter1 = df[df[column_name] == filter_value1]  # Rows matching filter_value1\n",
    "        df_filter2 = df[df[column_name] == filter_value2]  # Rows matching filter_value2\n",
    "\n",
    "        # Write the filtered data into separate sheets in a new Excel file\n",
    "        print(f\"[*] Writing filtered data to '{output_path}'...\")\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            df_filter1.to_excel(writer, sheet_name=f\"Filter_{filter_value1}\", index=False)\n",
    "            df_filter2.to_excel(writer, sheet_name=f\"Filter_{filter_value2}\", index=False)\n",
    "        print(\"[SUCCESS] File saved successfully.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] The file was not found: {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# --- How to use the script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths and parameters with your actual file paths and filter values\n",
    "    input_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    output_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    column_to_filter = \"CH TYPE\"  # Replace with the actual column name\n",
    "    filter_value_1 = \"W\"  # Replace with the first filter value\n",
    "    filter_value_2 = \"P\"  # Replace with the second filter value\n",
    "\n",
    "    filter_and_split_to_sheets(\n",
    "        input_path=input_excel_file,\n",
    "        output_path=output_excel_file,\n",
    "        column_name=column_to_filter,\n",
    "        filter_value1=filter_value_1,\n",
    "        filter_value2=filter_value_2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "041ee0f9-d1e3-4cec-8505-54ce6a8c82a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Reading Excel file: 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n",
      "[*] Filtering sheet: 'Filter_W'...\n",
      "[*] Writing all sheets to 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'...\n",
      "[SUCCESS] File saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_column_and_save(input_path: str, output_path: str, sheet_to_filter: str, column_name: str, filter_values: list):\n",
    "    \"\"\"\n",
    "    Filters rows based on multiple values in a specific column in a specific sheet and writes the result \n",
    "    to a single sheet while including other sheets unchanged in the output Excel file.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input Excel file.\n",
    "        output_path (str): Path to the output Excel file.\n",
    "        sheet_to_filter (str): Name of the sheet to apply the filter on.\n",
    "        column_name (str): Name of the column to apply the filter on.\n",
    "        filter_values (list): List of values to filter on.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the Excel file with all sheets\n",
    "        print(f\"[*] Reading Excel file: '{input_path}'\")\n",
    "        all_sheets = pd.read_excel(input_path, sheet_name=None, engine='openpyxl')  # Load all sheets into a dictionary\n",
    "\n",
    "        # Check if the specified sheet exists\n",
    "        if sheet_to_filter not in all_sheets:\n",
    "            print(f\"[ERROR] The sheet '{sheet_to_filter}' does not exist in the input file.\")\n",
    "            return\n",
    "\n",
    "        # Process the specific sheet\n",
    "        print(f\"[*] Filtering sheet: '{sheet_to_filter}'...\")\n",
    "        df_to_filter = all_sheets[sheet_to_filter]  # Get the specific sheet as a DataFrame\n",
    "\n",
    "        # Ensure the specified column exists in the sheet\n",
    "        if column_name not in df_to_filter.columns:\n",
    "            print(f\"[ERROR] The column '{column_name}' does not exist in the sheet '{sheet_to_filter}'.\")\n",
    "            return\n",
    "\n",
    "        # Filter rows based on the specified values\n",
    "        filtered_df = df_to_filter[df_to_filter[column_name].isin(filter_values)]  # Filter rows matching any of the values\n",
    "\n",
    "        # Update the sheet with the filtered data\n",
    "        all_sheets[sheet_to_filter] = filtered_df\n",
    "\n",
    "        # Write all sheets to the new Excel file\n",
    "        print(f\"[*] Writing all sheets to '{output_path}'...\")\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            for sheet_name, sheet_data in all_sheets.items():\n",
    "                sheet_data.to_excel(writer, sheet_name=sheet_name, index=False)  # Write each sheet to the output file\n",
    "        print(\"[SUCCESS] File saved successfully.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] The file was not found: {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# --- How to use the script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths and parameters with your actual file paths and filter values\n",
    "    input_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    output_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    sheet_to_filter = \"Filter_W\"  # Replace with the name of the sheet to filter\n",
    "    column_to_filter = \"COMMONPORT\"  # Replace with the actual column name\n",
    "    filter_values = [\"COMMON IN/OUT(11/12)\", \"COMMON IN/OUT(17/18)\", \"COMMON IN/OUT(23/24)\", \"COMMON IN/OUT(5/6)\"]  # Replace with the filter values\n",
    "\n",
    "    filter_column_and_save(\n",
    "        input_path=input_excel_file,\n",
    "        output_path=output_excel_file,\n",
    "        sheet_to_filter=sheet_to_filter,\n",
    "        column_name=column_to_filter,\n",
    "        filter_values=filter_values\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f99a0486-6208-43cf-9630-5d44a913a800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ciena Multi-Sheet Data Processing Script Initialized ---\n",
      "Reading all sheets from 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'...\n",
      "Successfully read 2 sheets: ['Filter_W', 'Filter_P']\n",
      "\n",
      "Processing sheet: 'Filter_P'...\n",
      "Initial number of rows in 'Filter_P': 4148\n",
      "Filtering complete. Removed 2065 row(s) where 'OPSNO' was blank.\n",
      "Final number of rows in 'Filter_P': 2083\n",
      "\n",
      "Saving all sheets to 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'...\n",
      "  - Writing sheet: 'Filter_W'\n",
      "  - Writing sheet: 'Filter_P'\n",
      "\n",
      "--- Process Completed Successfully! New file created. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_and_copy_excel_sheets(input_excel_path: str, output_excel_path: str, sheet_to_process: str, column_to_check: str):\n",
    "    \"\"\"\n",
    "    Reads an entire Excel workbook, filters a specific sheet to remove rows\n",
    "    where a particular column is blank, and saves the cleaned sheet along\n",
    "    with all other original sheets to a new Excel file.\n",
    "\n",
    "    Args:\n",
    "        input_excel_path (str): The full path to the source Excel file.\n",
    "        output_excel_path (str): The full path for the cleaned output Excel file.\n",
    "        sheet_to_process (str): The name of the sheet to clean.\n",
    "        column_to_check (str): The name of the column to check for blank values.\n",
    "    \"\"\"\n",
    "    print(\"--- Ciena Multi-Sheet Data Processing Script Initialized ---\")\n",
    "\n",
    "    # --- 1. Validate Input File ---\n",
    "    if not os.path.exists(input_excel_path):\n",
    "        print(f\"Error: The input file was not found at '{input_excel_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # --- 2. Read ALL sheets from the Excel file into a dictionary of DataFrames ---\n",
    "        # By setting sheet_name=None, pandas reads all sheets.\n",
    "        # The result is a dictionary where keys are sheet names and values are the DataFrames.\n",
    "        print(f\"Reading all sheets from '{input_excel_path}'...\")\n",
    "        all_sheets_dict = pd.read_excel(input_excel_path, sheet_name=None)\n",
    "        print(f\"Successfully read {len(all_sheets_dict)} sheets: {list(all_sheets_dict.keys())}\")\n",
    "\n",
    "        # --- 3. Check if the sheet to be processed actually exists ---\n",
    "        if sheet_to_process not in all_sheets_dict:\n",
    "            print(f\"Error: The sheet named '{sheet_to_process}' was not found in the Excel file.\")\n",
    "            print(f\"Available sheets are: {list(all_sheets_dict.keys())}\")\n",
    "            return\n",
    "\n",
    "        # --- 4. Get the specific DataFrame to clean ---\n",
    "        df_to_clean = all_sheets_dict[sheet_to_process]\n",
    "\n",
    "        # --- 5. Check if the specified column exists in the target sheet ---\n",
    "        if column_to_check not in df_to_clean.columns:\n",
    "            print(f\"Error: Column '{column_to_check}' not found in sheet '{sheet_to_process}'.\")\n",
    "            print(f\"Available columns in '{sheet_to_process}' are: {list(df_to_clean.columns)}\")\n",
    "            return\n",
    "\n",
    "        # --- 6. The Core Logic: Filter and Remove Blank Rows from the target sheet ---\n",
    "        print(f\"\\nProcessing sheet: '{sheet_to_process}'...\")\n",
    "        initial_rows = len(df_to_clean)\n",
    "        print(f\"Initial number of rows in '{sheet_to_process}': {initial_rows}\")\n",
    "\n",
    "        # The .notna() method identifies cells that are NOT blank (not NA/NaN).\n",
    "        cleaned_df = df_to_clean[df_to_clean[column_to_check].notna()]\n",
    "\n",
    "        final_rows = len(cleaned_df)\n",
    "        rows_removed = initial_rows - final_rows\n",
    "        print(f\"Filtering complete. Removed {rows_removed} row(s) where '{column_to_check}' was blank.\")\n",
    "        print(f\"Final number of rows in '{sheet_to_process}': {final_rows}\")\n",
    "\n",
    "        # --- 7. Update the dictionary with the cleaned DataFrame ---\n",
    "        # We replace the original DataFrame for the processed sheet with our new, cleaned one.\n",
    "        all_sheets_dict[sheet_to_process] = cleaned_df\n",
    "\n",
    "        # --- 8. Save ALL sheets to the new Excel file using ExcelWriter ---\n",
    "        # ExcelWriter is the tool for writing multiple sheets to one file.\n",
    "        print(f\"\\nSaving all sheets to '{output_excel_path}'...\")\n",
    "        with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
    "            for sheet_name, df in all_sheets_dict.items():\n",
    "                print(f\"  - Writing sheet: '{sheet_name}'\")\n",
    "                # Write each DataFrame (df) to a sheet with its original name (sheet_name)\n",
    "                # index=False prevents pandas from writing the row numbers as a column.\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "        print(\"\\n--- Process Completed Successfully! New file created. ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- MAIN EXECUTION BLOCK ---\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # --- CONFIGURE YOUR FILE AND COLUMN SETTINGS HERE ---\n",
    "\n",
    "    # 1. Set the full path to your input Excel file.\n",
    "    # Using a raw string (r'...') is helpful on Windows to avoid issues with backslashes.\n",
    "    input_file = r'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n",
    "\n",
    "    # 2. Set the name of the ONE sheet you want to clean.\n",
    "    # This must match the sheet name in the file exactly (it is case-sensitive).\n",
    "    sheet_name_to_clean = \"Filter_P\"\n",
    "\n",
    "    # 3. Set the name of the column to check for blank values within that sheet.\n",
    "    column_name_to_filter = \"OPSNO\"\n",
    "\n",
    "    # 4. Set the full path for your new, cleaned output file.\n",
    "    output_file = r'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n",
    "\n",
    "    # --- DO NOT EDIT BELOW THIS LINE ---\n",
    "    # Call the function with your settings\n",
    "    clean_and_copy_excel_sheets(\n",
    "        input_excel_path=input_file,\n",
    "        output_excel_path=output_file,\n",
    "        sheet_to_process=sheet_name_to_clean,\n",
    "        column_to_check=column_name_to_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30f37359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'. Found sheets: ['Filter_W', 'Filter_P']\n",
      "\n",
      "Processing sheets...\n",
      "  - Processing sheet: 'Filter_W'\n",
      "    -> Found 'Channel Project' column. Removed 10 row(s).\n",
      "  - Processing sheet: 'Filter_P'\n",
      "    -> Found 'Channel Project' column. Removed 10 row(s).\n",
      "\n",
      "Processing complete. Filtered data has been saved to 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_excel_sheets(input_filename, output_filename):\n",
    "    \"\"\"\n",
    "    Reads an Excel file, filters rows in each sheet based on a specific\n",
    "    value in the 'Channel Project' column, and saves to a new file.\n",
    "\n",
    "    Args:\n",
    "        input_filename (str): The name of the source Excel file.\n",
    "        output_filename (str): The name for the new, filtered Excel file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read all sheets from the input Excel file into a dictionary of DataFrames\n",
    "        # Setting sheet_name=None tells pandas to read all sheets\n",
    "        all_sheets_df = pd.read_excel(input_filename, sheet_name=None, engine='openpyxl')\n",
    "        print(f\"Successfully loaded '{input_filename}'. Found sheets: {list(all_sheets_df.keys())}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_filename}' was not found. Please make sure it's in the same directory as the script.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Define the column to filter and the value to remove\n",
    "    column_to_filter = \"Channel Project\"\n",
    "    value_to_remove = \"L0 Channel (FOTR)\"\n",
    "\n",
    "    # Create a Pandas Excel writer using openpyxl as the engine.\n",
    "    # The 'with' statement ensures the writer is properly closed.\n",
    "    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "        print(\"\\nProcessing sheets...\")\n",
    "        # Loop through the dictionary of sheets (sheet_name: DataFrame)\n",
    "        for sheet_name, df in all_sheets_df.items():\n",
    "            print(f\"  - Processing sheet: '{sheet_name}'\")\n",
    "\n",
    "            # Check if the target column exists in the current sheet's DataFrame\n",
    "            if column_to_filter in df.columns:\n",
    "                initial_rows = len(df)\n",
    "                \n",
    "                # Filter the DataFrame to keep rows where the column value is NOT the one to remove\n",
    "                filtered_df = df[df[column_to_filter] != value_to_remove]\n",
    "                \n",
    "                final_rows = len(filtered_df)\n",
    "                rows_removed = initial_rows - final_rows\n",
    "\n",
    "                print(f\"    -> Found '{column_to_filter}' column. Removed {rows_removed} row(s).\")\n",
    "                \n",
    "                # Write the filtered DataFrame to the new Excel file, preserving the sheet name\n",
    "                # index=False prevents pandas from writing the DataFrame index as a column\n",
    "                filtered_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "            else:\n",
    "                # If the column doesn't exist, write the original sheet to the new file without changes\n",
    "                print(f\"    -> Column '{column_to_filter}' not found. Copying sheet as is.\")\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"\\nProcessing complete. Filtered data has been saved to '{output_filename}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Change these file names to match your files\n",
    "    input_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    output_excel_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "    \n",
    "    # --- Run the function ---\n",
    "    filter_excel_sheets(input_excel_file, output_excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85189a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading data from Excel files...\n",
      "Primary file loaded successfully with 23 columns.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Worksheet named 'Link name reversed' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrimary file loaded successfully with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(original_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Load ONLY the required column from the reference file to save memory.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m df_reference = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREFERENCE_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREFERENCE_SHEET_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mREFERENCE_COLUMN_NAME\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Drop any empty rows from the reference column\u001b[39;00m\n\u001b[32m     48\u001b[39m df_reference.dropna(subset=[REFERENCE_COLUMN_NAME], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mangoel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     data = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mangoel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[39m, in \u001b[36mExcelFile.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1578\u001b[39m     sheet_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     **kwds,\n\u001b[32m   1597\u001b[39m ) -> DataFrame | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m \u001b[33;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mangoel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:773\u001b[39m, in \u001b[36mBaseExcelReader.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masheetname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(asheetname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     sheet = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43masheetname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume an integer if not a string\u001b[39;00m\n\u001b[32m    775\u001b[39m     sheet = \u001b[38;5;28mself\u001b[39m.get_sheet_by_index(asheetname)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mangoel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:582\u001b[39m, in \u001b[36mOpenpyxlReader.get_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_if_bad_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.book[name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mangoel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:624\u001b[39m, in \u001b[36mBaseExcelReader.raise_if_bad_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_if_bad_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sheet_names:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWorksheet named \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Worksheet named 'Link name reversed' not found"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "\n",
    "# 2. Configuration\n",
    "# --- File and Sheet Parameters ---\n",
    "PRIMARY_FILE_PATH = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "REFERENCE_FILE_PATH = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\WLP_NMS_services_UTL_comparsion_file_31_July_2025.xlsx\"\n",
    "\n",
    "PRIMARY_SHEET_NAME = 'Filter_W'\n",
    "REFERENCE_SHEET_NAME = 'Link name reversed'\n",
    "\n",
    "# --- Column Name Parameters ---\n",
    "PRIMARY_COLUMN_NAME = 'Link Name'\n",
    "REFERENCE_COLUMN_NAME = 'Unnamed: 4'\n",
    "\n",
    "\n",
    "# --- Helper Function for Standardization ---\n",
    "def create_canonical_key(city_string):\n",
    "\n",
    "    try:\n",
    "        # Split the string by ' - ' and strip whitespace from each part\n",
    "        parts = sorted([part.strip() for part in city_string.split(' - ', 1)])\n",
    "        return '|'.join(parts)\n",
    "    except:\n",
    "        # Return a non-matching key for blank or malformed cells\n",
    "        return None\n",
    "\n",
    "\n",
    "# 3. Data Loading\n",
    "print(\"Step 1: Loading data from Excel files...\")\n",
    "\n",
    "# Load the primary data file, keeping all original columns.\n",
    "df_primary = pd.read_excel(\n",
    "    io=PRIMARY_FILE_PATH,\n",
    "    sheet_name=PRIMARY_SHEET_NAME,\n",
    ")\n",
    "# Store the original column order to use for the final output\n",
    "original_columns = df_primary.columns.tolist()\n",
    "print(f\"Primary file loaded successfully with {len(original_columns)} columns.\")\n",
    "\n",
    "# Load ONLY the required column from the reference file to save memory.\n",
    "df_reference = pd.read_excel(\n",
    "    io=REFERENCE_FILE_PATH,\n",
    "    sheet_name=REFERENCE_SHEET_NAME,\n",
    "    usecols=[REFERENCE_COLUMN_NAME]\n",
    ")\n",
    "# Drop any empty rows from the reference column\n",
    "df_reference.dropna(subset=[REFERENCE_COLUMN_NAME], inplace=True)\n",
    "print(\"Reference file loaded.\")\n",
    "\n",
    "\n",
    "# 4. Transformation Logic\n",
    "print(\"Step 2: Processing and standardizing data...\")\n",
    "\n",
    "# --- Part A: Build the Canonical Mapping Dictionary ---\n",
    "# Create a highly efficient lookup map (dictionary).\n",
    "# The key is the standardized format (e.g., 'CityA|CityB').\n",
    "# The value is the desired format from the reference file (e.g., 'CityA - CityB').\n",
    "print(\"Building a reference map of correct connection names...\")\n",
    "canonical_map = {\n",
    "    create_canonical_key(name): name \n",
    "    for name in df_reference[REFERENCE_COLUMN_NAME]\n",
    "}\n",
    "\n",
    "# --- Part B: Apply the Transformation to the Primary DataFrame ---\n",
    "# Create the standardized key for the primary column.\n",
    "df_primary['canonical_key'] = df_primary[PRIMARY_COLUMN_NAME].apply(create_canonical_key)\n",
    "\n",
    "# Use the map to find the correct format.\n",
    "# If a key exists in our map, use the map's value; otherwise, keep the original name.\n",
    "# This preserves names that don't have a match in the reference file.\n",
    "df_primary[PRIMARY_COLUMN_NAME] = df_primary['canonical_key'].map(canonical_map).fillna(df_primary[PRIMARY_COLUMN_NAME])\n",
    "\n",
    "print(\"Data transformation complete. All other columns are preserved.\")\n",
    "\n",
    "\n",
    "# 5. Data Export\n",
    "print(\"Step 3: Preparing final output and updating the original Excel file...\")\n",
    "\n",
    "# --- Part A: Assemble the Final DataFrame ---\n",
    "# Select only the original columns in their original order.\n",
    "# This step drops the temporary 'canonical_key' helper column.\n",
    "df_final = df_primary[original_columns]\n",
    "\n",
    "# --- Part B: Export to Original Excel File ---\n",
    "# Use ExcelWriter in append mode with 'replace' to overwrite the specific sheet\n",
    "# while leaving other sheets in the workbook untouched.\n",
    "try:\n",
    "    with pd.ExcelWriter(\n",
    "        PRIMARY_FILE_PATH,\n",
    "        mode=\"a\",\n",
    "        engine=\"openpyxl\",\n",
    "        if_sheet_exists=\"replace\"\n",
    "    ) as writer:\n",
    "        df_final.to_excel(writer, sheet_name=PRIMARY_SHEET_NAME, index=False)\n",
    "    \n",
    "    print(f\"✅ Success! Sheet '{PRIMARY_SHEET_NAME}' in '{PRIMARY_FILE_PATH}' has been updated.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file was not found at {PRIMARY_FILE_PATH}. Please check the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8e4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading data from Excel files...\n",
      "Primary file loaded successfully with 23 columns.\n",
      "Reference file loaded.\n",
      "Step 2: Processing and standardizing data...\n",
      "Building a reference map of correct connection names...\n",
      "Data transformation complete. All other columns are preserved.\n",
      "Step 3: Preparing final output and updating the original Excel file...\n",
      "✅ Success! Sheet 'Filter_P' in 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx' has been updated.\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "\n",
    "# 2. Configuration\n",
    "# --- File and Sheet Parameters ---\n",
    "PRIMARY_FILE_PATH = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "REFERENCE_FILE_PATH = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\WLP_NMS_services_UTL_comparsion_file_31_July_2025.xlsx\"\n",
    "\n",
    "PRIMARY_SHEET_NAME = 'Filter_P'\n",
    "REFERENCE_SHEET_NAME = 'Link name reversed'\n",
    "\n",
    "# --- Column Name Parameters ---\n",
    "PRIMARY_COLUMN_NAME = 'Link Name'\n",
    "REFERENCE_COLUMN_NAME = 'Unnamed: 4'\n",
    "\n",
    "\n",
    "# --- Helper Function for Standardization ---\n",
    "def create_canonical_key(city_string):\n",
    "\n",
    "    try:\n",
    "        # Split the string by ' - ' and strip whitespace from each part\n",
    "        parts = sorted([part.strip() for part in city_string.split(' - ', 1)])\n",
    "        return '|'.join(parts)\n",
    "    except:\n",
    "        # Return a non-matching key for blank or malformed cells\n",
    "        return None\n",
    "\n",
    "\n",
    "# 3. Data Loading\n",
    "print(\"Step 1: Loading data from Excel files...\")\n",
    "\n",
    "# Load the primary data file, keeping all original columns.\n",
    "df_primary = pd.read_excel(\n",
    "    io=PRIMARY_FILE_PATH,\n",
    "    sheet_name=PRIMARY_SHEET_NAME,\n",
    ")\n",
    "# Store the original column order to use for the final output\n",
    "original_columns = df_primary.columns.tolist()\n",
    "print(f\"Primary file loaded successfully with {len(original_columns)} columns.\")\n",
    "\n",
    "# Load ONLY the required column from the reference file to save memory.\n",
    "df_reference = pd.read_excel(\n",
    "    io=REFERENCE_FILE_PATH,\n",
    "    sheet_name=REFERENCE_SHEET_NAME,\n",
    "    usecols=[REFERENCE_COLUMN_NAME]\n",
    ")\n",
    "# Drop any empty rows from the reference column\n",
    "df_reference.dropna(subset=[REFERENCE_COLUMN_NAME], inplace=True)\n",
    "print(\"Reference file loaded.\")\n",
    "\n",
    "\n",
    "# 4. Transformation Logic\n",
    "print(\"Step 2: Processing and standardizing data...\")\n",
    "\n",
    "# --- Part A: Build the Canonical Mapping Dictionary ---\n",
    "# Create a highly efficient lookup map (dictionary).\n",
    "# The key is the standardized format (e.g., 'CityA|CityB').\n",
    "# The value is the desired format from the reference file (e.g., 'CityA - CityB').\n",
    "print(\"Building a reference map of correct connection names...\")\n",
    "canonical_map = {\n",
    "    create_canonical_key(name): name \n",
    "    for name in df_reference[REFERENCE_COLUMN_NAME]\n",
    "}\n",
    "\n",
    "# --- Part B: Apply the Transformation to the Primary DataFrame ---\n",
    "# Create the standardized key for the primary column.\n",
    "df_primary['canonical_key'] = df_primary[PRIMARY_COLUMN_NAME].apply(create_canonical_key)\n",
    "\n",
    "# Use the map to find the correct format.\n",
    "# If a key exists in our map, use the map's value; otherwise, keep the original name.\n",
    "# This preserves names that don't have a match in the reference file.\n",
    "df_primary[PRIMARY_COLUMN_NAME] = df_primary['canonical_key'].map(canonical_map).fillna(df_primary[PRIMARY_COLUMN_NAME])\n",
    "\n",
    "print(\"Data transformation complete. All other columns are preserved.\")\n",
    "\n",
    "\n",
    "# 5. Data Export\n",
    "print(\"Step 3: Preparing final output and updating the original Excel file...\")\n",
    "\n",
    "# --- Part A: Assemble the Final DataFrame ---\n",
    "# Select only the original columns in their original order.\n",
    "# This step drops the temporary 'canonical_key' helper column.\n",
    "df_final = df_primary[original_columns]\n",
    "\n",
    "# --- Part B: Export to Original Excel File ---\n",
    "# Use ExcelWriter in append mode with 'replace' to overwrite the specific sheet\n",
    "# while leaving other sheets in the workbook untouched.\n",
    "try:\n",
    "    with pd.ExcelWriter(\n",
    "        PRIMARY_FILE_PATH,\n",
    "        mode=\"a\",\n",
    "        engine=\"openpyxl\",\n",
    "        if_sheet_exists=\"replace\"\n",
    "    ) as writer:\n",
    "        df_final.to_excel(writer, sheet_name=PRIMARY_SHEET_NAME, index=False)\n",
    "    \n",
    "    print(f\"✅ Success! Sheet '{PRIMARY_SHEET_NAME}' in '{PRIMARY_FILE_PATH}' has been updated.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file was not found at {PRIMARY_FILE_PATH}. Please check the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28fca8-462f-4983-9671-d0eaee3b441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ciena Excel Sheet Synchronizer (Multi-Sheet Safe) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mangoel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\mangoel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Source Sheet: 'Filter_P' -> Destination Sheet: 'Only Protect'\n",
      "Warning: Key column 'NE-SHELFSLOT-OPSSW' not found in the destination sheet 'Only Protect'. Creating an empty sheet.\n",
      "Using 'NE-SHELFSLOT-OPSSW' as the key for comparison.\n",
      "\n",
      "Found 2073 new row(s) to add to sheet 'Only Protect'.\n",
      "\n",
      "Processing Source Sheet: 'Filter_W' -> Destination Sheet: 'A end comparisonM&A'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mangoel\\AppData\\Local\\Temp\\ipykernel_28336\\3381045377.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_dest = pd.concat([df_dest, rows_to_add], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key column 'NE-SHELFSLOT-OPSSW' not found in the destination sheet 'A end comparisonM&A'. Creating an empty sheet.\n",
      "Using 'NE-SHELFSLOT-OPSSW' as the key for comparison.\n",
      "\n",
      "Found 2073 new row(s) to add to sheet 'A end comparisonM&A'.\n",
      "\n",
      "Saving updated data to 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\WLP_NMS_services_UTL_comparsion_file_31_July_2025.xlsx'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mangoel\\AppData\\Local\\Temp\\ipykernel_28336\\3381045377.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_dest = pd.concat([df_dest, rows_to_add], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! The destination file has been updated.\n",
      "\n",
      "--- Synchronization Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Please update these variables with your actual file names, sheet names, and column name.\n",
    "\n",
    "# Source File: The file you are copying FROM.\n",
    "source_file_path = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Output File.xlsx\"\n",
    "source_sheet_names = ['Filter_P', 'Filter_W']  # List of source sheets to process\n",
    "\n",
    "# Destination File: The file you are copying TO. This file will be modified.\n",
    "dest_file_path = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\WLP_NMS_services_UTL_comparsion_file_31_July_2025.xlsx\"\n",
    "dest_sheet_names = ['Only Protect', 'A end comparisonM&A']  # Corresponding destination sheets\n",
    "\n",
    "# Key Column: The column name that uniquely identifies a row and is present in both files.\n",
    "key_column_name = 'NE-SHELFSLOT-OPSSW'\n",
    "\n",
    "# --- End of Configuration ---\n",
    "\n",
    "\n",
    "def sync_excel_sheets(source_path, source_sheets, dest_path, dest_sheets, key_column):\n",
    "    \"\"\"\n",
    "    Compares specified sheets from two Excel files based on a key column and copies missing rows\n",
    "    from the source to the destination file. This function updates specified sheets while preserving\n",
    "    all other sheets and existing rows in the destination file.\n",
    "    \"\"\"\n",
    "    print(\"--- Ciena Excel Sheet Synchronizer (Multi-Sheet Safe) ---\")\n",
    "\n",
    "    # Validate input\n",
    "    if len(source_sheets) != len(dest_sheets):\n",
    "        print(\"Error: The number of source sheets must match the number of destination sheets.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the entire destination file to preserve all other sheets\n",
    "        dest_file_data = pd.read_excel(dest_path, sheet_name=None)\n",
    "\n",
    "        # Iterate through the specified sheets\n",
    "        for source_sheet, dest_sheet in zip(source_sheets, dest_sheets):\n",
    "            print(f\"\\nProcessing Source Sheet: '{source_sheet}' -> Destination Sheet: '{dest_sheet}'\")\n",
    "\n",
    "            # --- 1. Load Data from Excel Files ---\n",
    "            try:\n",
    "                df_source = pd.read_excel(source_path, sheet_name=source_sheet)\n",
    "                df_dest = dest_file_data.get(dest_sheet, pd.DataFrame(columns=df_source.columns))\n",
    "            except Exception as e:\n",
    "                print(f\"Error while reading sheets: {e}\")\n",
    "                return\n",
    "\n",
    "            # --- 2. Validate Key Column ---\n",
    "            if key_column not in df_source.columns:\n",
    "                print(f\"Error: Key column '{key_column}' not found in the source sheet '{source_sheet}'.\")\n",
    "                return\n",
    "            if key_column not in df_dest.columns:\n",
    "                print(f\"Warning: Key column '{key_column}' not found in the destination sheet '{dest_sheet}'. Creating an empty sheet.\")\n",
    "                df_dest = pd.DataFrame(columns=df_source.columns)\n",
    "\n",
    "            print(f\"Using '{key_column}' as the key for comparison.\")\n",
    "\n",
    "            # --- 3. Identify Rows to be Added ---\n",
    "            dest_keys = set(df_dest[key_column].unique())\n",
    "            rows_to_add = df_source[~df_source[key_column].isin(dest_keys)]\n",
    "\n",
    "            # --- 4. Append Data ---\n",
    "            if rows_to_add.empty:\n",
    "                print(f\"\\nResult: No new rows to add for sheet '{dest_sheet}'. The destination sheet is already up-to-date.\")\n",
    "            else:\n",
    "                num_rows = len(rows_to_add)\n",
    "                print(f\"\\nFound {num_rows} new row(s) to add to sheet '{dest_sheet}'.\")\n",
    "\n",
    "                df_dest = pd.concat([df_dest, rows_to_add], ignore_index=True)\n",
    "\n",
    "            # Update the sheet in the destination file data\n",
    "            dest_file_data[dest_sheet] = df_dest\n",
    "\n",
    "        # --- 5. Save Updated Data ---\n",
    "        try:\n",
    "            print(f\"\\nSaving updated data to '{dest_path}'...\")\n",
    "            with pd.ExcelWriter(dest_path, engine='openpyxl') as writer:\n",
    "                for sheet_name, data in dest_file_data.items():\n",
    "                    data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            print(\"Success! The destination file has been updated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving the file: {e}\")\n",
    "            return\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: The file {e.filename} was not found. Please check the file path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Synchronization Complete ---\")\n",
    "\n",
    "\n",
    "# --- Run the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    sync_excel_sheets(\n",
    "        source_file_path,\n",
    "        source_sheet_names,\n",
    "        dest_file_path,\n",
    "        dest_sheet_names,\n",
    "        key_column_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db931ef0-436c-4d9e-906d-5ce86e5143f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Utilization\\performance-measurements (10).csv\n",
      "Reading file: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Utilization\\performance-measurements (11).csv\n",
      "Reading file: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Utilization\\performance-measurements (12).csv\n",
      "Reading file: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Utilization\\performance-measurements (13).csv\n",
      "Reading file: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Utilization\\performance-measurements (14).csv\n",
      "Reading file: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Utilization\\performance-measurements (7).csv\n",
      "Reading file: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Utilization\\performance-measurements (8).csv\n",
      "Reading file: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Utilization\\performance-measurements (9).csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mangoel\\AppData\\Local\\Temp\\ipykernel_28336\\335753297.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended CSV saved to: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Appended.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def append_csv_files(folder_path, output_file):\n",
    "    dataframes = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            print(f\"Reading file: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Appended CSV saved to: {output_file}\")\n",
    "\n",
    "folder_path = r'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Utilization'\n",
    "output_file = r'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Appended.csv'\n",
    "append_csv_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62b8ec-271b-4246-a3a0-a4e73a875c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Appended.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv(file_path, start_column_index):\n",
    "    \"\"\"\n",
    "    Process the input CSV file to add a 'reference' column and calculate the max numerical value\n",
    "    between the specified starting column and the last column with values.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        start_column_index (int): The index of the starting column (e.g., column 'I').\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame with low_memory=False\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    # Create the 'reference' column by combining values from specified columns\n",
    "    df['reference'] = df[['Network element', 'Measurement point', 'Parameter', 'Direction']].apply(\n",
    "        lambda row: '_'.join(row.values.astype(str)), axis=1\n",
    "    )\n",
    "\n",
    "    # Get the column names from the starting column index to the last column\n",
    "    columns_with_values = df.iloc[:, start_column_index:].select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Calculate the maximum numerical value for each row between the specified columns\n",
    "    df['Max'] = df[columns_with_values].max(axis=1)\n",
    "\n",
    "    # Save the updated DataFrame back to the same CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"Updated CSV file saved: {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Appended.csv'  # Replace with the path to your CSV file\n",
    "start_column_index = 8  # Replace with the column index for 'I' (e.g., 8 for the 9th column)\n",
    "process_csv(file_path, start_column_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe48cc-9519-4253-8bec-21f6afdc6bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully copied to sheet 'PMcombine-June2025' in 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\WLP_NMS_services_UTL_comparsion_file_31_July_2025.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "\n",
    "def update_excel_from_csv(target_file, source_csv, target_sheet_name, columns_to_copy):\n",
    "    try:\n",
    "        # Load the Target Excel File\n",
    "        target_wb = openpyxl.load_workbook(target_file)\n",
    "        \n",
    "        # Check if the target sheet exists\n",
    "        if target_sheet_name not in target_wb.sheetnames:\n",
    "            print(f\"Sheet '{target_sheet_name}' does not exist in the target file.\")\n",
    "            return\n",
    "        \n",
    "        # Load the Target Sheet\n",
    "        target_sheet = target_wb[target_sheet_name]\n",
    "        \n",
    "        # Clear all data in the target sheet\n",
    "        target_sheet.delete_rows(1, target_sheet.max_row)\n",
    "        \n",
    "        # Load the Source CSV File\n",
    "        source_data = pd.read_csv(source_csv, low_memory=False)\n",
    "        \n",
    "        # Check if all required columns exist in the source CSV\n",
    "        missing_columns = [col for col in columns_to_copy if col not in source_data.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns in source CSV file: {missing_columns}\")\n",
    "            return\n",
    "        \n",
    "        # Write the headers to the target sheet\n",
    "        for col_idx, column_name in enumerate(columns_to_copy, start=1):\n",
    "            target_sheet.cell(row=1, column=col_idx, value=column_name)\n",
    "        \n",
    "        # Copy data from the source CSV to the target sheet\n",
    "        for row_idx, row in source_data.iterrows():\n",
    "            for col_idx, column_name in enumerate(columns_to_copy, start=1):\n",
    "                target_sheet.cell(row=row_idx + 2, column=col_idx, value=row[column_name])\n",
    "        \n",
    "        # Save changes to the target file\n",
    "        target_wb.save(target_file)\n",
    "        print(f\"Data successfully copied to sheet '{target_sheet_name}' in '{target_file}'.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Define file paths and parameters\n",
    "target_file = r'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\WLP_NMS_services_UTL_comparsion_file_31_July_2025.xlsx'\n",
    "source_csv = r'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\Appended.csv'\n",
    "target_sheet_name = \"PMcombine-June2025\"\n",
    "columns_to_copy = ['Network element', 'Measurement point', 'Parameter', 'Direction', 'reference', 'Max']\n",
    "\n",
    "# Call the function\n",
    "update_excel_from_csv(target_file, source_csv, target_sheet_name, columns_to_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd19ab-30c4-4b0d-a2ab-186aa5ee807d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully copied to sheet 'new' in 'C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\OPTICAL_SERVICES_REPORT_V2.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "def add_new_sheet_from_csv(csv_file_path, excel_file_path, new_sheet_name, columns_to_copy):\n",
    "    try:\n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_file_path, low_memory=False)\n",
    "        \n",
    "        # Check if all required columns exist in the CSV file\n",
    "        missing_columns = [col for col in columns_to_copy if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns in CSV file: {missing_columns}\")\n",
    "            return\n",
    "        \n",
    "        # Filter the DataFrame to include only the specified columns\n",
    "        filtered_df = df[columns_to_copy]\n",
    "        \n",
    "        # Check if the Excel file exists\n",
    "        try:\n",
    "            wb = load_workbook(excel_file_path)\n",
    "        except FileNotFoundError:\n",
    "            # If the file doesn't exist, create a new workbook\n",
    "            wb = Workbook()\n",
    "            wb.remove(wb.active)  # Remove the default sheet\n",
    "        \n",
    "        # Create a new sheet in the Excel file\n",
    "        if new_sheet_name in wb.sheetnames:\n",
    "            print(f\"Sheet '{new_sheet_name}' already exists. Overwriting it.\")\n",
    "            del wb[new_sheet_name]\n",
    "        \n",
    "        new_sheet = wb.create_sheet(title=new_sheet_name)\n",
    "        \n",
    "        # Write the DataFrame to the new sheet\n",
    "        for col_idx, column_name in enumerate(columns_to_copy, start=1):\n",
    "            new_sheet.cell(row=1, column=col_idx, value=column_name)  # Write headers\n",
    "        \n",
    "        for row_idx, row in filtered_df.iterrows():\n",
    "            for col_idx, value in enumerate(row, start=1):\n",
    "                new_sheet.cell(row=row_idx + 2, column=col_idx, value=value)  # Write data\n",
    "        \n",
    "        # Save the updated Excel file\n",
    "        wb.save(excel_file_path)\n",
    "        print(f\"Data successfully copied to sheet '{new_sheet_name}' in '{excel_file_path}'.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Define file paths and parameters\n",
    "csv_file_path = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\OPTICAL_SERVICES_REPORT_V2.csv\"  # Replace with the actual path to your CSV file\n",
    "excel_file_path = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\OPTICAL_SERVICES_REPORT_V2.xlsx\"  # Replace with the actual path to your Excel file\n",
    "new_sheet_name = \"new\"\n",
    "columns_to_copy = ['Name', 'Layer rate qualifier', 'Endpoint', 'Endpoint1']\n",
    "\n",
    "# Call the function\n",
    "add_new_sheet_from_csv(csv_file_path, excel_file_path, new_sheet_name, columns_to_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8e014-3abc-4b24-b4c4-eae26f89f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_excel(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    filtered_df = df[(df['Name'].notna()) & (df['Layer rate qualifier'] == '100GE')]\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='w') as writer:\n",
    "        filtered_df.to_excel(writer, index=False, sheet_name='Filtered Data')\n",
    "    print(\"Done.\")\n",
    "file_path = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\OPTICAL_SERVICES_REPORT_V2.xlsx\"\n",
    "filter_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4506b-e007-4c7e-b294-e371bbc3d6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the new Excel file: C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\OPTICAL_SERVICES_REPORT_V2_.xlsx\n",
      "Original rows: 9364, New rows: 18728\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def stack_endpoints(input_file, output_file):\n",
    "    # Load the Excel file into a Pandas DataFrame\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Check if the required columns exist in the input file\n",
    "    required_columns = ['Name', 'Layer rate qualifier', 'Endpoint', 'Endpoint1']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"The input file must contain the following columns: {required_columns}\")\n",
    "\n",
    "    # Replace missing values in 'Endpoint 1' and 'Endpoint 2' with a placeholder (e.g., 'N/A')\n",
    "    df['Endpoint'] = df['Endpoint'].fillna('N/A')\n",
    "    df['Endpoint1'] = df['Endpoint1'].fillna('N/A')\n",
    "\n",
    "    # Create a new DataFrame by stacking 'Endpoint 1' and 'Endpoint 2'\n",
    "    stacked_df = pd.DataFrame({\n",
    "        'Name': pd.concat([df['Name'], df['Name']], ignore_index=True),\n",
    "        'Layer rate qualifier': pd.concat([df['Layer rate qualifier'], df['Layer rate qualifier']], ignore_index=True),\n",
    "        'Endpoint': pd.concat([df['Endpoint'], df['Endpoint1']], ignore_index=True)\n",
    "    })\n",
    "\n",
    "    # Save the new DataFrame to a new Excel file\n",
    "    stacked_df.to_excel(output_file, index=False, sheet_name='Stacked Data')\n",
    "\n",
    "    print(f\"Successfully created the new Excel file: {output_file}\")\n",
    "    print(f\"Original rows: {len(df)}, New rows: {len(stacked_df)}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\OPTICAL_SERVICES_REPORT_V2.xlsx\"  # Replace with your input file path\n",
    "output_file = r\"C:\\Users\\mangoel\\OneDrive - Ciena Corporation\\WLP_automation_folder\\OPTICAL_SERVICES_REPORT_V2_.xlsx\"  # Replace with your desired output file name\n",
    "stack_endpoints(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf8d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
